---
title: "Readme"
author: "Carter Hoffman"
date: " `r Sys.Date()` "

header-includes:
   - \usepackage{amsmath,amssymb}
   - \usepackage[bitstream-charter]{mathdesign}
   - \usepackage[T1]{fontenc}
   
output: 
  md_document:
    variant: gfm

---
```{r, eval=FALSE, echo=FALSE}
output: 
  pdf_document:
      toc: true
      toc_depth: 2
      df_print: kable
    
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    theme: readable
    highlight: tango

# knit directly to md for github
output:
  md_document:
    variant: gfm
```


```{r echo=FALSE}
knitr::opts_chunk$set(tidy = FALSE, warning = FALSE, message = FALSE, max.print = 5)
```

```{r echo=FALSE}
# load libraries
library(tidyverse)
library(wesanderson)
library(patchwork)
```

# Assigning taxonomy

blah

## primer abbreviations

There are 78 sequences generated for each predicted amplicon. Abbreviations for the predicted amplicons were used in the filenames for convienience, and are listed in the table below.

| region | primer | abbreviation |
|-----------|-------|--------|
|V1-V3| A17F-515R|k17|
|V4-V6 |515F-1114R|k515|
|V2-V3| 16S_BV2f-16S_BV3r|bbv|
|V4|F515-R806|cap|
|V3-V4|V3F-V4R|gras|
|V6 | v6_1183F-v6_1410R|v6|
|V3 | v3_579F-v3_779R|v3|

```{r, fig.height=4, fig.width=8, echo=FALSE}
s16_70 <- read_tsv("../../../resources/specific_windows_weighted_08_03_1426/ktw_16s_w70.csv", col_names = FALSE)

adj <- 35

vstart <- c(78,191,528,675,925,1093,1237,1369,1566)
vstop <- c(151,333,596,783,986,1163,1297,1424,1606)

pstart <- c(19,635,154,435,616,434,452,982)
pend <- c(633,1234,596,909,911,892,635,1203)
rank_pos <- c(.1, .2, .3, .4, .5, .6, .7, .8)

vr_pos <- .85

swa_plot <- ggplot(s16_70, aes(x=X1, y=X2)) +
    geom_rect(xmin=vstart[1]-adj, xmax=vstop[1]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[2]-adj, xmax=vstop[2]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[3]-adj, xmax=vstop[3]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[4]-adj, xmax=vstop[4]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[5]-adj, xmax=vstop[5]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[6]-adj, xmax=vstop[6]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[7]-adj, xmax=vstop[7]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[8]-adj, xmax=vstop[8]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[9]-adj, xmax=vstop[9]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  annotate("text", x = 80, y = .95, label = "V1", color="gray40", size=5) +
  annotate("text", x = 230, y = .95, label = "V2", color="gray40", size=5) +
  annotate("text", x = 530, y = .95, label = "V3", color="gray40", size=5) +
  annotate("text", x = 700, y = .95, label = "V4", color="gray40", size=5) +
  annotate("text", x = 920, y = .95, label = "V5", color="gray40", size=5) +
  annotate("text", x = 1100, y = .95, label = "V6", color="gray40", size=5) +
  annotate("text", x = 1230, y = .95, label = "V7", color="gray40", size=5) +
  annotate("text", x = 1360, y = .95, label = "V8", color="gray40", size=5) +
  annotate("text", x = 1550, y = .95, label = "V9", color="gray40", size=5) +
  geom_line(size=1.1) +
  theme_classic() +
  scale_x_continuous(breaks=seq(0,1700,400), limits = c(0,1700))+ 
  scale_y_continuous(breaks=seq(0,1,.2), limits = c(.001,1))+ 
  theme(plot.title = element_text(size=20), plot.subtitle = element_text(size=14), axis.title.y= element_text(size=16), axis.title.x = element_text(size=16), axis.text = element_text(size=14), legend.title = element_text(size=14),legend.text = element_text(size=14), strip.text.x = element_text(size = 14), strip.text.y = element_text(size = 14)) +
  labs(x="Position in alignment", y="Entropy")


amp_plot <- ggplot(s16_70, aes(x=X1, y=X2)) +
  geom_rect(xmin=vstart[1]-adj, xmax=vstop[1]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[2]-adj, xmax=vstop[2]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[3]-adj, xmax=vstop[3]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[4]-adj, xmax=vstop[4]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[5]-adj, xmax=vstop[5]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[6]-adj, xmax=vstop[6]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[7]-adj, xmax=vstop[7]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[8]-adj, xmax=vstop[8]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
  geom_rect(xmin=vstart[9]-adj, xmax=vstop[9]-adj, ymin=-1, ymax=Inf, fill="ivory3") +
    annotate("text", x = 80, y = vr_pos, label = "V1", color="gray40", size=5) +
  annotate("text", x = 230, y = vr_pos, label = "V2", color="gray40", size=5) +
  annotate("text", x = 530, y = vr_pos, label = "V3", color="gray40", size=5) +
  annotate("text", x = 700, y = vr_pos, label = "V4", color="gray40", size=5) +
  annotate("text", x = 920, y = vr_pos, label = "V5", color="gray40", size=5) +
  annotate("text", x = 1100, y = vr_pos, label = "V6", color="gray40", size=5) +
  annotate("text", x = 1230, y = vr_pos, label = "V7", color="gray40", size=5) +
  annotate("text", x = 1360, y = vr_pos, label = "V8", color="gray40", size=5) +
  annotate("text", x = 1550, y = vr_pos, label = "V9", color="gray40", size=5) +
   #bv
  geom_segment(aes(x = pstart[3], y = rank_pos[2], xend = pend[3], yend = rank_pos[2]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="16S_BV2f-16S_BV3r", x=1000, y=rank_pos[2]), size=4) +
  #geom_text(aes(label="Bukin 2019", x=950, y=rank_pos[2]), size=5) +
  geom_text(aes(label="V2-V3", x=700, y=rank_pos[2]), size=5) +
  
  #gras
  geom_segment(aes(x = pstart[6], y = rank_pos[5], xend = pend[6], yend = rank_pos[5]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="V3F-V4R", x=1200, y=rank_pos[5]), size=4) +
  #geom_text(aes(label="Graspeuntner 2018", x=1350, y=rank_pos[5]), size=5) +
  geom_text(aes(label="V3-V4", x=1000, y=rank_pos[5]), size=5) +
  
  #v3
  geom_segment(aes(x = pstart[7], y = rank_pos[3], xend = pend[7], yend = rank_pos[3]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="V3_579F-V3_779R", x=1070, y=rank_pos[3]), size=4) +
  geom_text(aes(label="V3", x=700, y=rank_pos[3]), size=5) +
  
  #cap
  geom_segment(aes(x = pstart[5], y = rank_pos[4], xend = pend[5], yend = rank_pos[4]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="F515-R806", x=1250, y=rank_pos[4]), size=4) +
  #geom_text(aes(label="Caporaso 2011", x=1300, y=rank_pos[4]), size=5) +
  geom_text(aes(label="V4", x=950, y=rank_pos[4]), size=5) +
  
  #k17
  geom_segment(aes(x = pstart[1], y = rank_pos[1], xend = pend[1], yend = rank_pos[1]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="A17F-515R", x=900, y=rank_pos[1]), size=4) +
  #geom_text(aes(label="Komesu 2017", x=1000, y=rank_pos[1]), size=5) +
  geom_text(aes(label="V1-V3", x=750, y=rank_pos[1]), size=5) +
  
  #b646
  #geom_segment(aes(x = pstart[4], y = rank_pos[6], xend = pend[4], yend = rank_pos[6]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="MiCSQ_343FL-MiCSQ_806R", x=1400, y=rank_pos[6]), size=4) +
  #geom_text(aes(label="Bukin 2019", x=1250, y=rank_pos[6]), size=5) +
  #geom_text(aes(label="V3-V5", x=1000, y=rank_pos[6]), size=5) +
  
  #k515
  geom_segment(aes(x = pstart[2], y = rank_pos[6], xend = pend[2], yend = rank_pos[6]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="515F-1114R", x=1600, y=rank_pos[7]), size=4) +
  #geom_text(aes(label="Komesu 2017", x=1650, y=rank_pos[7]), size=5) +
  geom_text(aes(label="V4-V6", x=1350, y=rank_pos[6]), size=5) +
  
  #v6
  geom_segment(aes(x = pstart[8], y = rank_pos[7], xend = pend[8], yend = rank_pos[7]), color = wes_palette("Darjeeling2")[5], size=2) +
  #geom_text(aes(label="V6_1183F-V6_1410R", x=1700, y=rank_pos[8]), size=4) +
  geom_text(aes(label="V6", x=1280, y=rank_pos[7]), size=5) +
  
  theme_classic() +
  theme(plot.title = element_text(size=20), plot.subtitle = element_text(size=14), axis.title.y= element_text(size=16), axis.title.x = element_blank(), axis.text = element_text(size=14), legend.title = element_text(size=14),legend.text = element_text(size=14), strip.text.x = element_text(size = 14), strip.text.y = element_text(size = 14)) +
  theme(axis.ticks.y = element_blank()) +
  theme(axis.text.y = element_blank()) +
  #, axis.text.x=element_text(angle = 30, hjust = 1)) +
  scale_x_continuous(breaks=seq(0,1600,200)) +
  #labs(title="Position of amplicons on the 16S rRNA gene", x="Position in 16S gene sequence", y="Targeted amplicons")
  labs(x="Position in 16S gene sequence", y="Targeted amplicons")
```

```{r, fig.height=8, fig.width=8}
amp_plot / swa_plot + plot_annotation(tag_levels = 'A')
```

## Classification

### BLCA

All predicted amplicon data was classified in batch by using a bash script and run in a Linux environment. In general, BLCA was called in this manner when using the default NCBI 16S database:

```
python [path to BLCA script] -i [path to input FASTA file] -o [new name to save the output file under]
```

For the remaining databases, the path to the database and taxonomy file had to be included:

```
python [path to BLCA script] -i [path to input FASTA file] --tax [path to taxonomy file] --db [path to the database] -o [new name to save the output file under]
```

Just as an example, part of the `bash` file is reproduced below. You'll have to change your bash file to reflect where your resource files are located.

```{bash, eval=FALSE}
#!/usr/bin/env bash

# batch script for KTW type strains

# the BLCA default 16S database
python 2.blca_main.py -i extracted_16s_regions_2019-11-26/extracted_cap_tpstr_2019-11-26_1550.fna -o cap_tpstr_default_2019-11-26.outfile
python 2.blca_main.py -i extracted_16s_regions_2019-11-26/extracted_gras_tpstr_2019-11-26_1551.fna -o gras_tpstr_default_2019-11-26.outfile
#   remaining 6 predicted amplicon files would be continued here 

# the greengenes database
python 2.blca_main.py -i extracted_16s_regions_2019-11-26/extracted_cap_tpstr_2019-11-26_1550.fna --tax gg/gg_13_5_taxonomy.taxonomy --db gg/gg_13_5 -o cap_tpstr_gg_2019-11-26.outfile
python 2.blca_main.py -i extracted_16s_regions_2019-11-26/extracted_gras_tpstr_2019-11-26_1551.fna --tax gg/gg_13_5_taxonomy.taxonomy --db gg/gg_13_5 -o gras_tpstr_gg_2019-11-26.outfile
#   remaining 6 predicted amplicon files would be continued here 
```

#### reformat results

The evaluation steps need the classification results to be in a different format than the standard BLCA output. We used the script `blcaout_to_csv.py` to do this reformatting, where `../raw_data/blca_tpstr_2019-11-26` is the path to the folder of classified results.

```
(base) C:\Users\Carter\Documents\thesis\thesis_git\taxonomy\src_files>python blcaout_to_csv.py -i ../raw_data/blca_tpstr_2019-11-26
```

### Qiime

Qiime needed some extra formatting and importing steps before the predicted amplicons could be classified. The classification was also done in a Linux environment, and all work was done in a folder that included the data, compiled classifier, and bash scripts.

#### data prep

All the FASTA files had to be imported as a Qiime object before the classifier would work. We used this bash script to import all the files in a directory. One important note is that the sequences must be DNA. If you're using RNA sequences, replace the Uracil (U) nucleotides with Thymine (T) (we just used the search and replace function in a plain text program, like Window's Notebook).

We stuck to a filename convention when dealing with all the predicted amplicons, for example `extracted_b646_tpstr_2019-11-26_1552_dna.fna`. The bash script will read in all file in a directory expecting this convention, and name the output file based on the second and third positions in the snake-case filename. So the output would be `b646_tpstr_sequence.qza`.

```{bash, eval=FALSE}
#!/bin/bash

# get the files from command line arg
seq_files=($1*.fna)

for x in ${seq_files[@]}
do
    # backticks return the commands as a variable
    # cut splits a string, returns the 2&3 element
    get_basename=`basename "$x" | cut -d'_' -f2,3`
    
    qiime tools import --type 'FeatureData[Sequence]' --input-path $x --output-path $1$get_basename"_sequence"
done
```

The bash script was called like this:

```{shellsession}
(qiime2-2019.10):~/qiime_classification_2019-12-18$ ./batch_qiime_import.bash extract_vr_dna_2019-12-18_50/
```

#### classify

The Naive Bayes classifier requires a trained database. "Trained" in this context means a FASTA multirecord file of sequences that have been broken into a bazillion 8-mers, and the number of specific 8-mers found in each taxon enumerated and a probabiity calculated. "Training" the Silva database took 4 days on the OHSU Advanced Computing Cluster. The example bash script that performs the classification of all predicted amplicons using the NCBI 16S database is below. When using any other database, change the `--i-classifier` option to reflect the path to the trained database.

```{bash, eval=FALSE}
#!/bin/bash

# get the files from command line arg
seq_files=($1*.qza)

for x in ${seq_files[@]}
do
    # backticks return the commands as a variable
    # cut splits a string, returns the 2&3 element
    get_basename=`basename "$x" | cut -d'_' -f1,2`
    qiime feature-classifier classify-sklearn --i-classifier ncbi16s_classifier_2019-12-17.qza --i-reads $x --o-classification $1$get_basename"_classified" --p-confidence 0
done
```

#### extract qza files

Qiime’s .qza files are simply .zip files. They have several extra files for metadata, but the buisiness end of it is the file `taxonomy.tsv`. Again, using a bash script is slightly more convienient than renaming the .qza to .zip and then going through the unzipping process by hand.

```{bash, eval=FALSE}
#!/bin/bash

# get the files from command line arg
seq_files=($1*_classified.qza)

for x in ${seq_files[@]}
do
    qiime tools extract --input-path $x --output-path $2
done
```

The bash script was called like this:

```{shellsession}
(qiime2-2019.10):~/qiime_classification_2019-12-18$ ./batch_qiime_extract.bash extract_vr_dna_2019-12-18_50/ final_data/
```

#### reformat results

Like BLCA, the evaluation step needs the classification output to be in a specific format. The `blcaout_to_csv.py` script was modified to do this with the Qiime results, and is called `qiimeout_to_csv.py`. `../raw_data/qiime_outfiles_2019-12-18/` is the path to the output Qiime results.

```
(base) >qiimeout_to_csv.py -i ../raw_data/qiime_outfiles_2019-12-18/
```

## Synonyms

Finally, the reformatted results had to be checked for bacterial species synonyms.

### Prokaryotic Nomenclature Up-to-Date

Prokaryotic synonyms were downloaded from the [Prokaryotic Nomenclature Up-to-Date website](https://www.dsmz.de/services/online-tools/prokaryotic-nomenclature-up-to-date/downloads) run by the DSMZ-German Collection of Microorganisms and Cell Cultures, GmbH. Strains are out of scope, so entries like _Enterobacter cloacae cloacae_ and _Enterobacter cloacae dissolvens_ are considered synonyms of _Enterobacter cloacae_. Misspellings that don't map to known synonyms are assigned as a mismatch, but unless you're typing your own taxonomy into FASTA records, the chance of misspellings are low.

### in general

Any genus or species taxon labled as "unidentified" or "uncultured" is designated as "unavailable". "Unclassified" is reserved for when the classification algo thinks there's no match for the record. "Unavailable" just means there's no information for that record, and are assigned as true non-matches.

Input is the formatted _csv_ file as written by the python scripts `blcaout_to_csv.py` and `qiimeout_to_csv.py`, for example:

```
id,taxon,rank,confidence
CP011325.22226.23780,Bacteria,domain,100.0
```

the `--map_file` option (`-m`) takes a path to a file that can be used to map accession numbers to taxonomy by `newick_tools.inhouse_to_species.py`, found in the `resources` directory. 

  * master_wellcome_list_2019-05-07.csv
  
  * type_strains_map_2019-11-26.csv

```
>python validate_match.py -i path/to/formatted/assigned_taxonomy_file.csv -m path/to/map/file
```

The screen output is a little verbose, but unless the script finds a discrepency it will only print the output filename. An example of a successfully resolving a synonym is:

```{shellsession}
        the result name 'Streptococcus thermophilus' was found to be a synonym linked to the current name 'Streptococcus salivarius'
        comparing the mapped name 'Streptococcus salivarius' to the current name 'Streptococcus salivarius'
        and found to be the same, writing [AY188352.1.1546,Streptococcus salivarius subsp. salivarius,Streptococcus salivarius,33.2333333333,1] to file

writing to ../processed_files/validated_outfiles_2020-01-05_47/validated_formatted_k515_tpstr_default_2019_2020-01-05_47.csv
```

and one that did not:

```{shellsession}
        the result name 'Streptococcus thermophilus' was found to be a synonym linked to the current name 'Streptococcus salivarius'
        comparing the mapped name 'Streptococcus anginosus' to the current name 'Streptococcus salivarius'
        and found to be different, writing [AFIM01000033.206.1753,Streptococcus anginosus SK52 = DSM 20563,Streptococcus salivarius,53.5833333333,0] to file
```




