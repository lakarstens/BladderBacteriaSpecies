knitr::opts_chunk$set(tidy = FALSE, warning = FALSE, message = FALSE, max.print = 5)
# load libraries
library(tidyverse)
library(wesanderson)
library(broom)
library(viridis)
library(ggrepel)
library(votesys)
#---------------------------#
#   functions!              #
#---------------------------#
list_of_df <- function(path_to_dir) {
# got tired of loading all the csv files individually,
# so this function does it in batch. Reads all filenames
# in a dir, then creates a df from the csv, then adds names
# to the growing list of dataframes.
get_filenames <- list.files(path_to_dir, full.names = TRUE, pattern="validated_formatted*")
build_csv_df <- lapply(get_filenames, read.csv, stringsAsFactors=FALSE)
grow_names <- vector()
for (g in get_filenames) {
name_of_var <- paste(strsplit(g, "_")[[1]][6], strsplit(g, "_")[[1]][7], sep="-")
new_name <- paste(strsplit(g, "_")[[1]][3], name_of_var, sep="_")
grow_names <- c(grow_names, new_name)
}
names(build_csv_df) <- grow_names
return(build_csv_df)
}
f1_records <- function(results, db, vr) {
# seperate the confusion matrix cells first
d <- ifelse(results$match==1, 1, 0)
b <- ifelse((results$match==0 & results$blca!="unidentified"), 1, 0)
c <- ifelse(results$blca=="unidentified", 1,0)
results_confusion <- cbind(results, d,b,c)
# calculate plain F1 scores
f1_values <- double()
w_values <- double()
howmany <- integer()
for (x in unique(results$confidence)) {
smaller <- results_confusion %>% filter(confidence >= x)
f1 <- 2*sum(smaller$d)/((2*sum(smaller$d)+sum(smaller$c)+sum(smaller$b)))
w <- (sum(smaller$d) + sum(smaller$c)) / (sum(smaller$b) + sum(smaller$c) + 2*sum(smaller$d))
f1_values <- c(f1_values, f1)
w_values <- c(w_values, w)
howmany <- c(howmany, sum(smaller$match))
}
w_min=min(w_values)
w_max=max(w_values)
# now calculate the normalized F1 scores
normalized_f1 <- double()
normalized_w <- double()
for (x in unique(results_confusion$confidence)) {
smaller <- results_confusion %>% filter(confidence >= x)
w <- (sum(smaller$d) + sum(smaller$c)) / (sum(smaller$b) + sum(smaller$c) + 2*sum(smaller$d))
nw <- (w-w_min)/(w_max-w_min)
nrecall <- sum(smaller$d)/(sum(smaller$c) + sum(smaller$d))
nprecision <- sum(smaller$d)/(sum(smaller$b) + sum(smaller$d))
nf1 <- (nrecall * nw) + (nprecision *(1-nw))
normalized_f1 <- c(normalized_f1, nf1)
normalized_w <- c(normalized_w, nw)
}
#print(sprintf("f1 used these: %s, %s", db, vr))
return(data.frame(f1_scores=f1_values, f1_weights=w_values, norm_weights=normalized_w, norm_f1=normalized_f1, database=db, var_region=vr, true_positives=howmany, confidence=unique(results$confidence)))
}
total_results <- function(each_result) {
each_name <- names(each_result)
final_db <- data.frame(f1_scores=double(), f1_weights=double(), database=character(), var_region=character())
for (x in each_name) {
#print(sprintf("sending to f1 %s, %s, %s", x, strsplit(x, split="_")[[1]][1], strsplit(x,split="_")[[1]][2]))
holder <- f1_records(each_result[x][[1]], strsplit(x, split="_")[[1]][1], strsplit(x,split="_")[[1]][2])
final_db <- rbind(final_db, holder)
}
# hard coding a line here for the ffh results
# final_db$var_region = factor(final_db$var_region, levels=c("k13", "b23", "v3", "b34", "g34", "c4", "k36", "v6", "ffh"))
# and the regular 16S results
#final_db$var_region = factor(final_db$var_region, levels=c("k13", "b23", "v3", "b34", "g34", "c4", "k36", "v6"))
return(final_db)
}
build_median <- function(results_of_full, each_name, n_size) {
final_db <- data.frame()
for (x in each_name) {
#print(sprintf("using these names: %s, %s, %s", x, strsplit(x, split="_")[[1]][1], strsplit(x,split="_")[[1]][2]))
take <- results_of_full %>% filter(database==strsplit(x, split="_")[[1]][1] & var_region==strsplit(x,split="_")[[1]][2]) %>% select(f1_scores)
s_take <- sort(take[[1]])
# this is cool.
# findInterval() searches a vector w for the price-is-right
#    (closest but not to exceed) value of x
get_median <- findInterval(median(s_take), s_take)
median_row <- results_of_full %>% filter(database==strsplit(x, split="_")[[1]][1] & var_region==strsplit(x,split="_")[[1]][2]) %>% filter(f1_scores==s_take[get_median])
# somehow there are multiple rows for two of the databases
# select the one with the largest number of pred positives
if (nrow(median_row) > 1) {
median_row <- median_row[which.max(median_row$true_positives),]
}
final_db <- rbind(final_db, median_row)
}
get_perc <- as.vector(mapply(function(x) x/n_size, final_db %>% select(true_positives)))
final_db$percent <- get_perc
return(final_db)
}
genomic_results <- list_of_df("../processed_files/validated_ncbi_091919/")
silva_results <- list_of_df("../processed_files/validated_silva_091819/")
greengenes_results <- list_of_df("../processed_files/validated_gg_092319/")
default_results <- list_of_df("../processed_files/validated_default_copy/")
all_res_db <- c(genomic_results, silva_results, default_results, greengenes_results)
all_results <- total_results(all_res_db)
# still need to supply the names of the databases
# using the inital list of dataframes
get_all_names <- names(all_res_db)
use_medians <- build_median(all_results, get_all_names, 150)
#ffh_res_db <- list(ncbi_k13, ncbi_k36, ncbi_b23, ncbi_b34, ncbi_c4, ncbi_g34, ncbi_v3, ncbi_v6, ncbi_ffh)
output_full <- total_results(all_res_db)
head(output_full)
#write.csv(output_full, file="blca_f1_values_2019-xx.csv")
# and make a genomic dataframe with ffh
#ffh_output_full <- total_results(ffh_res_db, list_with_ffh)
#write.csv(ffh_output_full, file="blca_with_ffh_f1_values_2019-xx.csv")
#full <- read.csv("blca_f1_values_2019-11-21.csv")
#full$var_region = factor(full$var_region, levels=c("k13", "b23", "v3", "b34", "g34", "c4", "k36", "v6"))
# load the ffh file
#full_ffh <- read.csv("blca_with_ffh_f1_values_2019-11-23.csv")
#full_ffh$var_region = factor(full_ffh$var_region, levels=c("k13", "b23", "v3", "b34", "g34", "c4", "k36", "v6", "ffh"))
head(all_results)
# full_ffh
color_scheme <- get_votes %>% filter(chamber=="house" & prefix=="HCR")
knitr::opts_chunk$set(tidy = FALSE, warning = FALSE, message = FALSE, max.print = 5)
# load libraries
library(tidyverse)
get_meas <- read_delim("catch_names_October2420_15_47.tsv", "\t", col_names = c("measureprefix", "measurenumber", "sessionkey", "committee", "catchline", "link"), escape_double=FALSE)
glimpse(get_meas)
ggplot(get_meas, aes(x=measureprefix)) +
geom_bar() +
facet_wrap(~sessionkey, nrow=7, ncol=4) +
scale_y_log10() +
theme(plot.title = element_text(size=20), plot.subtitle = element_text(size=14), axis.title.y= element_text(size=12), axis.title.x = element_text(size=12), axis.text = element_text(size=12), legend.position = "none", strip.text.x = element_text(size = 14), strip.text.y = element_text(size = 14), axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title="Number of measures found in each legislative assembly", x="Type of measure", y="number of measures, log scale")
gather_votes <- function(path_to_dir) {
# got tired of loading all the csv files individually,
# so this function does it in batch. Reads all filenames
# in a dir, then creates a df from the csv, then adds names
# to the growing list of dataframes.
# get the filenames
get_filenames <- list.files(path_to_dir, full.names = TRUE)
# read them into dataframes
build_csv_df <- lapply(get_filenames, read.csv, stringsAsFactors=FALSE)
grow_names <- vector()
for (g in get_filenames) {
new_name <- paste(strsplit(basename(g), "_")[[1]][2], strsplit(basename(g), "_")[[1]][3], sep="_")
grow_names <- c(grow_names, new_name)
}
names(build_csv_df) <- grow_names
large_df <- data.frame(measure=character(), representative=character(), party=character(), chamber=character(), vote=character(), stringsAsFactors = FALSE)
for (df in build_csv_df) {
large_df <- rbind(large_df, df)
}
return(large_df)
}
get_votes <- gather_votes("2019R1_votes")
head(get_votes)
get_votes <- get_votes %>% separate(measure, c("assembly", "prefix", "number"))
get_votes <- get_votes %>% unite(measure, prefix:number, remove=FALSE)
head(get_votes)
ggplot(get_votes %>% filter(chamber=="house"), aes(y=representative, x=vote)) +
geom_point(size=2)+
facet_wrap(~party)+
labs(title="House, 2019R1")
ggplot(get_votes %>% filter(chamber=="senate"), aes(y=representative, x=vote)) +
geom_point(size=2)+
facet_wrap(~party)+
labs(title="Senate, 2019R1")
color_scheme <- get_votes %>% filter(chamber=="house" & prefix=="HCR")
ggplot(color_scheme %>% filter((representative %in% c("Post", "Power", "Smith Warner", "Reardon")) & number==38)) +
geom_point(aes(y=representative, x=measure, color=party, fill=party), size=22, shape=22) +
geom_point(aes(y=representative, x=measure, color=vote, fill=vote), size=14, shape=22) +
scale_fill_manual(labels = c("absent", "democrat", "excused", "vote no", "republican", "vote yes"),values=c("black", "gray40", "gray90", "red", "gray65", "green3", "purple", "orange")) +
scale_color_manual(labels = c("absent", "democrat", "excused", "vote no", "republican", "vote yes"),values=c("white", "white", "white", "white", "white", "white", "white", "white", "white")) +
theme_bw()+
theme(legend.title = element_blank(), axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title="Self-indulgent color scheme", x="", y="")
ggplot(get_meas, aes(x=measureprefix)) +
geom_bar() +
facet_wrap(~sessionkey, nrow=7, ncol=4) +
scale_y_log10() +
ylim(1,1000) +
theme(plot.title = element_text(size=20), plot.subtitle = element_text(size=14), axis.title.y= element_text(size=12), axis.title.x = element_text(size=12), axis.text = element_text(size=12), legend.position = "none", strip.text.x = element_text(size = 14), strip.text.y = element_text(size = 14), axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title="Number of measures found in each legislative assembly", x="Type of measure", y="number of measures, log scale")
ggplot(get_meas, aes(x=measureprefix)) +
geom_bar() +
facet_wrap(~sessionkey, nrow=7, ncol=4) +
ylim(1,1000) +
scale_y_log10() +
theme(plot.title = element_text(size=20), plot.subtitle = element_text(size=14), axis.title.y= element_text(size=12), axis.title.x = element_text(size=12), axis.text = element_text(size=12), legend.position = "none", strip.text.x = element_text(size = 14), strip.text.y = element_text(size = 14), axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title="Number of measures found in each legislative assembly", x="Type of measure", y="number of measures, log scale")
scale_y_log10()
?scale_y_log10
ggplot(get_meas, aes(x=measureprefix)) +
geom_bar() +
facet_wrap(~sessionkey, nrow=7, ncol=4) +
scale_y_log10(limits=c(1,1000)) +
theme(plot.title = element_text(size=20), plot.subtitle = element_text(size=14), axis.title.y= element_text(size=12), axis.title.x = element_text(size=12), axis.text = element_text(size=12), legend.position = "none", strip.text.x = element_text(size = 14), strip.text.y = element_text(size = 14), axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title="Number of measures found in each legislative assembly", x="Type of measure", y="number of measures, log scale")
house_only <- get_votes %>% filter(chamber=="house" & party!="z_warning")
house_only %>% filter(prefix!="HCR" & prefix!="SCR") %>% select(prefix) %>% unique()
senate_only <- get_votes %>% filter(chamber=="senate" & party!="z_warning")
senate_only %>% filter(prefix!="HCR" & prefix!="SCR") %>% select(prefix) %>% unique()
house_2017_only <- get_2017_votes %>% filter(chamber=="senate" & party!="z_warning")
get_2017_votes <- gather_votes("2017R1_votes")
get_2017_votes <- get_2017_votes %>% separate(measure, c("assembly", "prefix", "number"))
get_2017_votes <- get_2017_votes %>% unite(measure, prefix:number, remove=FALSE)
head(get_2017_votes)
house_2017_only <- get_2017_votes %>% filter(chamber=="senate" & party!="z_warning")
house_2017_only %>% filter(prefix!="HCR" & prefix!="SCR") %>% select(prefix) %>% unique()
senate_2017_only <- get_2017_votes %>% filter(chamber=="senate" & party!="z_warning")
senate_2017_only %>% filter(prefix!="HCR" & prefix!="SCR") %>% select(prefix) %>% unique()
knitr::opts_chunk$set(tidy = FALSE, warning = FALSE, message = FALSE, max.print = 5)
# load libraries
library(tidyverse)
library(wesanderson)
library(lubridate)
library(ShortRead)
library(scales)
library(phyloseq)
runtime <- paste(Sys.Date(), hour(Sys.time()), minute(Sys.time()), sep='-')
ps_v4_ktw_blca_silva <- read_rds("../processed_files/ps_v4_ktw_blca_silva_2020-04-08.rds")
ps_v4_ktw_blca_ncbi16 <- read_rds("../processed_files/ps_v4_ktw_blca_ncbi16_2020-04-15.rds")
ps_v4_ktw_blca_gg <- read_rds("../processed_files/ps_v4_ktw_blca_gg_2020-04-15.rds")
glimpse(ps_v4_ktw_blca_silva)
knitr::opts_chunk$set(tidy = FALSE, warning = FALSE, message = FALSE, max.print = 5)
# load libraries
library(tidyverse)
data_set_a <- matrix(data=c(0,1,1,1,0,0), nrow=3, ncol=2)
data_set_b <- matrix(data=c(2,2,0,2,0,2), nrow=3, ncol=2)
data_set_a
data_set_b
data_set_a + data_set_b
