% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/centralEvaluation.r
\name{total_results}
\alias{total_results}
\title{total results}
\usage{
total_results(each_result)
}
\arguments{
\item{each_result}{The output from \code{list_of_df()}}
}
\value{
A dataframe of evaluated classification scheme results.

Dataframe column dictionary
\itemize{
\item true_match (numeric): Number of true matches (TM) in the dataset
\item false_match (numeric): Number of false matches (FM) in the dataset
\item cellC (numeric): The number of missed matches (MM)
\item recall (numeric): Recall is calculated by TM/(MM + TM)
\item precision (numeric): Precision is calculated by TM/(FM + TM)
\item fmeasure (numeric): F1 is calculated by (weight * R) + ((1-weight) * P)
\item weight (numeric): F1 can be weighted toward either Precision or Recall, but is equally weighted in this study
\item weight_r (numeric): Intended to use, but wound up ignoring
\item weight_p (numeric): Intended to use, but wound up ignoring
\item database (character): Database used
\item classifier (character): Classifier used
\item region (character): Identifier used
\item confidence (numeric): The number of times the same identification occured under random sampling (bootstrapping)
}
}
\description{
Iteriavly feeds a list of dataframes into \code{f1_records()}
}
